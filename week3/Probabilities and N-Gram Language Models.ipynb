{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Message Spam Detection\n",
    "\n",
    "\n",
    "## The Business Use Case\n",
    "\n",
    "You are the CEO of a new email service company trying to attract capital for your next growth stage. Most of the private equity firms you have spoken to want to see a user base of at least 100,000 users prior to commiting. Due to your superb marketing team, you estimate that each day you attract 1000 new users. Typically, 10% of all email messages are spam, and an average user receives 50 emails per day.\n",
    "\n",
    "One of your data scientists says he can provide you a model with **90% accuracy** in classifying spam / ham. Another data scientist says she can build a model that is only **80% accurate** but has **100% recall**. A third data scientist says he can build a model that has **80% accuracy** with **100% precision**.\n",
    "\n",
    "Which of the above models would you pick to model? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"spam-sms.csv\", encoding='latin-1')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[\"class\"].values\n",
    "X = data[\"text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Rule\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For our purposes, we will redefine this as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(spam|text) = \\frac{P(text|spam)P(spam)}{P(text)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here,\n",
    "\n",
    "- **prior** means before seeing any new text (evidence). Our impression of the likelihood of certain words appearing before new evidence is introduced.\n",
    "- **text** is the new message (evidence) being introduced that we want to classify as either spam or ham. Let's say we have a new text message `When are you coming home? I'm hungry.`.\n",
    "\n",
    "- $P(text)$ is the **prior likelihood** of seeing a particular text message with that exact combination of words. For instance, `P(\"the car is\")` will be significantly higher than `P(\"Downstream supply chain agents\")`, especially in the **context of text messages**.\n",
    "- $P(spam)$ is the likelihood that any text message will be spam. This is computed in our dataset:\n",
    "```python\n",
    "p_spam = sum(data[\"class\"] == \"spam\") / len(data)\n",
    "p_ham = 1 - p_spam # since there are only two classes\n",
    "```\n",
    "- $P(text|spam)$ is our **likelihood**. More specifically, the likelihood of this text message given that it is a piece of spam. It is saying, *let's assume that this message is spam. Knowing that, how likely is it that we'll find this particular combination of words in the text message?*\n",
    "\n",
    "In order to quickly get our likelihoods, we'll need to create a **likelihood table**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = data[data[\"class\"] == \"spam\"]\n",
    "ham_data = data[data[\"class\"] == \"ham\"]\n",
    "print(f\"The shape of the spam_data is {spam_data.shape}. The shape of the ham_data is {ham_data.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data[\"text\"].values)\n",
    "\n",
    "# create the vocabulary list\n",
    "vocabulary = set(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary list\n",
    "import spacy, string\n",
    "# nlp = spacy.load('en') <-- if spacy is giv\n",
    "from nltk import word_tokenize\n",
    "\n",
    "likelihood_table = pd.DataFrame(columns=[\"spam\", \"ham\"], index=list(vocabulary)).fillna(0)\n",
    "\n",
    "# populate the spam column in our likelihood table\n",
    "for i, sentence in enumerate(spam_data[\"text\"].values):\n",
    "    for token in word_tokenize(sentence):\n",
    "        if token.lower() not in likelihood_table.index:\n",
    "            likelihood_table.loc[token.lower(), \"spam\"] = 1\n",
    "        else:\n",
    "            likelihood_table.loc[token.lower(), \"spam\"] += 1\n",
    "likelihood_table.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the spam column in our likelihood table\n",
    "for i, sentence in enumerate(ham_data[\"text\"].values):\n",
    "    for token in nlp(sentence):\n",
    "        if token.text.lower() not in likelihood_table.index:\n",
    "            likelihood_table.loc[token.text.lower(), \"ham\"] = 1\n",
    "        else:\n",
    "            likelihood_table.loc[token.text.lower(), \"ham\"] += 1\n",
    "likelihood_table.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about words like `won`?\n",
    "```python\n",
    "likelihood_table.loc[\"won\"]\n",
    "```\n",
    "Output:\n",
    "```\n",
    "spam    73.0\n",
    "ham      0.0\n",
    "Name: won, dtype: float64\n",
    "```\n",
    "\n",
    "What is $P(w = won|c = ham)$? If even one of the words' class-conditional probabilities is 0, then the entire likelihood will be zero, since the likelihood is simply the product of all the words' individual likelihoods.\n",
    "\n",
    "# Additive Smoothing Techniques\n",
    "\n",
    "We can define a new likelihood for the word `won`:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P_{new}(w = won | c = ham) = \\frac{N_{ham, won} + \\alpha}{N_{ham} + \\alpha d}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $N_{ham, won}$ is the number of times `won` appears in a text message that is classified as ham, and $N_{ham}$ is simply the total number of messages that are classified as ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Optimizations to Improve Naive Bayes Probabilistic Models for Text Classification\n",
    "\n",
    "- to may be useful to simply create a simple **co-occurence matrix**, and **run a correlation analysis** on the features (words). If certain words have extremely high correlations, you may wish to take them out, or fuse them into a single entity.\n",
    "- apply smoothing techniques to handle **out-of-vocabulary test words**\n",
    "- **ensemble techniques like bagging / boosting** do **not** help. There isn't any \"variation\" in a Naive Bayes model. Given the same trained corpus $C$, and a new text message $m$, a Naive Bayes model will always output the same prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Words as Probabilities\n",
    "\n",
    "We can represent a sentence (a sequence of words) mathematically as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "w = \\{{w_0, w_1, w_2, \\dots,w_{s-1}}\\}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, **$s$** represents the total number of words in the sentence. **$w_{0}$** represents the first word in the sentence, **$w_{1}$** represents the second word in the sentence, and so on.\n",
    "\n",
    "# Exercise:\n",
    "\n",
    "`Older people, like everyone else, can benefit from accessing ride-sharing, but many are not comfortable with smart-phones.`\n",
    "\n",
    "You can ignore punctuation and capitalization for now.\n",
    "\n",
    "1. What is $s$?\n",
    "2. What is $w_4$? What is $w_6$?\n",
    "3. What is $V$ (this corpus' vocabulary size, assuming this is the only sentence in the corpus)? You can do this the hard way, by counting manually.\n",
    "\n",
    "```python\n",
    "sentence = \"Older people, like everyone else, can benefit from accessing ride-sharing, but many are not comfortable with smart-phones\"\n",
    "\n",
    "import re # the most efficient, concise way (less readable)\n",
    "vocabulary = set([re.sub(r'[^\\w\\s]','',word).lower() for word in sentence.split()])\n",
    "print(\"The size of the vocabulary is {} words\".format(vocabulary))\n",
    "```\n",
    "\n",
    "# Independence\n",
    "\n",
    "In statistics, two events are independent if the outcome of one event does not affect the probability of the outcomes of another event.\n",
    "\n",
    "![https://en.wikipedia.org/wiki/Independence_(probability_theory)](images/prob_independence.svg)\n",
    "\n",
    "You will also often see this written as \n",
    "$$\n",
    "\\begin{equation}\n",
    "P(A,B) = P(A) * P(B)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In other words, an event A is independent of event B if the **probability of event A and event B happening together** is equal to **the probability of event A multplied by the probability of event B**.\n",
    "\n",
    "![https://en.wikipedia.org/wiki/Independence_(probability_theory)](images/conditional-probability.png)\n",
    "\n",
    "# Bigram Model\n",
    "\n",
    "A bigram is a group of two tokens (frequently words) that are treated as one distinct entity. For instance, the distinct bigrams in the sentence `I am home now` would be\n",
    "```python\n",
    "bigrams = [\n",
    "    (\"I\", \"am\"),\n",
    "    (\"am\", \"home\"),\n",
    "    (\"home\", \"now\")\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "### Exercise:\n",
    "Write a Python function to find all the bigrams in the sentence\n",
    "`In recent years, Johnson & Johnson has been focusing more on its high-margin pharmaceutical segment via acquisitions.`\n",
    "\n",
    "**Hints**:\n",
    "- split the sentence into a list of individual words (`my_sentence.split()`)\n",
    "- remove punctuation\n",
    "- lowercase all the letters\n",
    "- use a for loop to iterate through this list, getting the **i-th** and **i + 1-th** elements of the list\n",
    "\n",
    "**Challenge**:\n",
    "Generalize this function to work with `n-grams`.\n",
    "\n",
    "## Language Model\n",
    "\n",
    "Are words in a sentence conditionally independent from each other? In other words, does knowing that the first word `The` change your belief in the likelihood of the second word that follows?\n",
    "\n",
    "Which of the following sentences is more likely?\n",
    "\n",
    "```python\n",
    "sentence_A = \"Jack went to Wal-Mart.\"\n",
    "sentence_B = \"at and the be of I\"\n",
    "```\n",
    "Notice that all the words in sentence B come from [Wikipedia most common words in the English language](https://en.wikipedia.org/wiki/Most_common_words_in_English). Yet we intuitively know that the sentence is nonsensical and is unlikely to be seen in natural language.\n",
    "\n",
    "We can express the likelihood of a sentence $w$ as $p(w)$, and define it as\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(w) = \\prod_{i=0}^{s}p(w_{i+1}|w_{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If we want to generalize this to an **N-Gram** model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(w) = \\prod_{i=0}^{s}p(w_i |w_{i-n+1}, w_{i-n+2}, \\dots, w_{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(w) = \\prod_{i=0}^{s}p(w_i | w_{i-n}^{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $w_{i-n}^{i} = w_{i-n+1}, w_{i-n+2}, \\dots, w_{i}$.\n",
    "\n",
    "## Model Evaluation: Choosing n in an n-Gram Model\n",
    "\n",
    "- the larger the dataset, and by implication, the more rich the corpus, the larger the n we can likely try.\n",
    "- in practice, $n = 2$, $n = 3$, $n = 4$ work well. A larger $n$ tends to begin to overfit (and may be computationally extremely expensive). Remember the **bias-variance** tradeoff:\n",
    "![http://scott.fortmann-roe.com/docs/BiasVariance.html](images/biasvariance.png)\n",
    "Here, as $n \\rightarrow \\infty$, model complexity increases dramatically.\n",
    "- **tune $n$ based on the performance of the downstream model**: usually n-gram models are the first step in a broader sentiment analysis prediction model, or topic modelling model, recommendation system, or sequence-to-sequence translation task.\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Look again at the definition of likelihood for a particular sentence:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L = p(w) = \\prod_{i=0}^{s}p(w_i | w_{i-n}^{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $w_{i-n}^{i}$ stands for $w_{i-n}, w_{i-n+1}, ..., w_{i-1}$ (if you have bi-gram (`n=2`) model, then you would have $w_{i-2}, w_{i-1}$)\n",
    "\n",
    "Is it reasonable to compare two sentences, one with $s=4$ (sentence length of 4 words) with the one below in Sentence B?\n",
    "\n",
    "##### Sentence A:\n",
    "> *I love to eat.*\n",
    "\n",
    "##### Sentence B:\n",
    "> *My escort was an exceptionally genial sixty-seven-year-old man named Don Seely, an electrical engineer who said that he was between jobs and using the unwanted free time to volunteer his services to the Northern Kentucky Tea Party, the rally’s host organization, as a Webmaster.*\n",
    "\n",
    "Answer: **No**. A common way of quantifying the likelihood of your n-gram models, accounting for different sizes of test corpuses, is to use **perplexity**. Remember that our likelihood of seeing a particular sentence is \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P = \\frac{1}{\\sqrt[N]{p(w)}}\n",
    "\\end{equation}\n",
    "$$\n",
    "$N$ is the length of all the words in the corpus. We typically use perplexity, instead of simply likelihood, as the overall model evaluation metric, because in general, **in order to compare two different models**, they should be using the same test corpus / vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perplexity(likelihood, N):\n",
    "    return 1 / (likelihood ** (1/N))\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(5, 500):\n",
    "    x.append(i)\n",
    "    y.append(perplexity(likelihood=.204, N=i)) # an example likelihood of .004\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Out-of-Vocabulary Words\n",
    "\n",
    "Let's pretend that our training corpus is\n",
    "\n",
    "> *This is mistaken logic. It is true that a high variance and low bias model can preform well in some sense.*\n",
    "\n",
    "Our test corpus is\n",
    "\n",
    "> *This **is not** true.*\n",
    "\n",
    "Assuming a bi-gram model is used, what is the **perplexity** of our model? \n",
    "\n",
    "We don't actually need to count each bi-gram. **The answer is infinity**. Why?\n",
    "$\n",
    "\\begin{equation}\n",
    "p(w_i = not | w_{i-1} = is) = 0\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "What you can do instead:\n",
    "\n",
    "* Look at the **frequency distribution** of words in your corpus\n",
    "* Decide upon some **threshold cutoff**, where every word below that threshold frequency will be converted into an `UNKNOWN` token. Now, whenever a new word appears that is out of vocabulary, you simply convert it into `UNKNOWN` and run the tests as usual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
